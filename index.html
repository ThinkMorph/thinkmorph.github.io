<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning">
  <meta name="keywords" content="ThinkMorph, Multimodal Reasoning, Chain-of-Thought, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .hero-body {
      padding: 3rem 1.5rem;
    }
    .publication-title {
      margin-bottom: 1rem;
    }
    .case-carousel-container {
      position: relative;
      margin: 2rem auto;
      max-width: 800px;
    }
    .case-carousel {
      overflow: hidden;
      border-radius: 10px;
      box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .case-carousel-track {
      display: flex;
      transition: transform 0.5s ease-in-out;
    }
    .case-item {
      min-width: 100%;
      cursor: pointer;
      position: relative;
    }
    .case-item img {
      display: block;
      width: 100%;
      height: auto;
      border-radius: 10px;
    }
    .carousel-nav {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      background: rgba(255,255,255,0.9);
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      font-size: 24px;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 2px 8px rgba(0,0,0,0.2);
      z-index: 10;
    }
    .carousel-nav:hover {
      background: white;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .carousel-nav.prev {
      left: -60px;
    }
    .carousel-nav.next {
      right: -60px;
    }
    .carousel-dots {
      display: flex;
      justify-content: center;
      gap: 10px;
      margin-top: 1rem;
    }
    .carousel-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #d1d5db;
      cursor: pointer;
      transition: all 0.3s ease;
    }
    .carousel-dot.active {
      background: #667eea;
      transform: scale(1.2);
    }
    .case-counter {
      text-align: center;
      margin-top: 1rem;
      font-size: 1rem;
      color: #6b7280;
    }
    .modal-content-custom {
      max-width: 90%;
      max-height: 90vh;
      margin: auto;
    }
    .gradient-text {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/thinkmorph_logo_new.png" alt="ThinkMorph Logo" style="max-width: 350px; margin-bottom: 1.5rem;">
          <h1 class="title is-3">Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning</h1>
          
          <div class="is-size-5 publication-authors" style="margin-top: 2rem;">
            <span class="author-block">
              <a href="mailto:kuvvius@gmail.com">Jiawei Gu</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=0SPl0hcAAAAJ&hl=zh-CN">Yunzhuo Hao</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=P8MF9AQAAAAJ&hl=en">Huichen Will Wang</a><sup>3,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a><sup>3,*</sup>,</span>
            <span class="author-block">
              <a href="#">Michael Qizhe Shieh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.yejinchoi.com/">Yejin Choi</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://www.ranjaykrishna.com/">Ranjay Krishna</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://ych133.github.io/">Yu Cheng</a><sup>5</sup></span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 1rem;">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>Zhejiang University,</span>
            <span class="author-block"><sup>3</sup>University of Washington,</span>
            <span class="author-block"><sup>4</sup>Stanford University,</span>
            <span class="author-block"><sup>5</sup>The Chinese University of Hong Kong</span>
          </div>

          <div class="is-size-6 has-text-centered" style="margin-top: 0.5rem;">
            <p>*Equal contribution</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/ThinkMorph/ThinkMorph" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ThinkMorph" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="font-size:18px">ðŸ¤—</span>
                  <span>Models & Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal reasoning requires <strong>iterative coordination between language and vision</strong>, 
            yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and 
            image thoughts should function as <strong>complementary, rather than isomorphic, modalities</strong> 
            that mutually advance reasoning. Guided by this principle, we build <strong>ThinkMorph</strong>, a unified model fine-tuned on 
            <strong>~24K high-quality interleaved reasoning traces</strong> spanning tasks with varying visual 
            engagement. ThinkMorph learns to generate progressive textâ€“image reasoning steps that concretely 
            manipulate visual content while maintaining coherent verbal logic. It delivers <strong class="gradient-text">large gains on vision-centric benchmarks</strong> 
            (averaging <strong>34.7%</strong> over the base model) and generalizes to out-of-domain tasks, 
            matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits <strong>emergent multimodal intelligence</strong>, including unseen 
            visual manipulation skills, adaptive switching between reasoning modes, and better test-time 
            scaling through diversified multimodal thoughts.
            These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/thinkmorph_main_web.png" alt="ThinkMorph Multimodal Reasoning" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            ThinkMorph is a unified multimodal model that generates <strong>interleaved textâ€“image reasoning steps</strong> to solve complex multimodal problems. Unlike conventional Chain-of-Thought approaches that only produce textual tokens, ThinkMorph can generate both image tokens and text tokens, resulting in complementary multimodal thoughts that jointly advance reasoning.
          </p>
          <p style="margin-top: 1rem;">
            We fine-tune ThinkMorph on <strong>~24K high-quality interleaved reasoning traces</strong> spanning four representative tasks with varying levels of visual engagement:
          </p>
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li><strong>Jigsaw Assembly:</strong> Determining correct arrangement of scrambled image patches</li>
            <li><strong>Spatial Navigation:</strong> Finding safe routes through grid maps with obstacles</li>
            <li><strong>Visual Search:</strong> Locating and identifying target objects in images</li>
            <li><strong>Chart Refocus:</strong> Answering questions about data visualizations</li>
          </ul>
          <p style="margin-top: 1rem;">
            Each interleaved sequence is carefully designed so that text and images function as <strong>complementary modalities</strong> that progressively guide reasoning toward solutions, rather than isomorphic representations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Visualization -->
<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Test-Time Scaling Comparison</h2>
    <div class="hero-body has-text-centered">
      <img src="./static/images/tts_mode_comparison_horizontal_v3.png" alt="Test-Time Scaling" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
      <p class="subtitle has-text-centered" style="margin-top: 1rem;">
        ThinkMorph's interleaved reasoning (green) consistently outperforms text-only (orange) and 
        vision-only (blue) modes across multiple benchmarks during test-time scaling.
      </p>
    </div>
  </div>
</section>

<!-- Emergent Properties -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Emergent Properties</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #667eea;">
            <h3 class="title is-4" style="color: #667eea;">Property â‘ : Unseen Visual Manipulations</h3>
            <p>
              ThinkMorph develops accurate and meaningful visual manipulations <strong>unseen in training data</strong> when generalizing to out-of-domain tasks. These include zoom-in operations, inpainting, multi-box generation, motion forecasting, perspective transformation, and region cropping. These emergent behaviors are precise and task-effective, contributing directly to problem solving. For example, when asked "Is the bell pepper red or yellow?", the model automatically generates a zoomed-in view to distinguish subtle color differences, mirroring human visual inspection without explicit prompting.
            </p>
          </div>
          
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #764ba2;">
            <h3 class="title is-4" style="color: #764ba2;">Property â‘¡: Autonomous Mode Switching</h3>
            <p>
              Despite being trained exclusively on interleaved data, ThinkMorph <strong>autonomously switches</strong> to text-only reasoning in 5.3% of inference cases. This switching is task-adaptive, not arbitrary: the model recognizes when fine-grained visual details are essential versus when textual reasoning alone suffices. On questions where the model switches to text-only mode, accuracy improves by 7.29% over interleaved reasoning, demonstrating effective dynamic allocation of reasoning effort based on task demands.
            </p>
          </div>
          
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #f093fb;">
            <h3 class="title is-4" style="color: #f093fb;">Property â‘¢: Better Test-Time Scaling via Diversified Thoughts</h3>
            <p>
              Interleaved reasoning enables superior test-time scaling by generating <strong>diversified thoughts</strong> that explore broader multimodal solution spaces. Under Best-of-N sampling, interleaved reasoning consistently outperforms unimodal approaches across all benchmarks, with substantial gains (e.g., +8.0% on BLINK-J). The scaling advantage arises from richer trajectory diversity: while unimodal reasoning chains are confined to single representational spaces, interleaved reasoning explores both modalities simultaneously, greatly improving the likelihood of discovering correct solutions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            ThinkMorph delivers <strong>substantial gains</strong> across diverse vision-centric benchmarks:
          </p>
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li><strong>Average improvement of 34.74%</strong> over the base model Bagel-7B across all benchmarks</li>
            <li><strong>85.84% improvement</strong> on Spatial Navigation (VSP)</li>
            <li><strong>38.75% improvement</strong> on Jigsaw Assembly (VisPuzzle)</li>
            <li><strong>Average gain of 20.74%</strong> over the base model on out-of-domain benchmarks</li>
          </ul>
          <p style="margin-top: 1rem;">
            Despite being fine-tuned on only 24K samples, ThinkMorph achieves performance comparable to, and in several cases exceeding, models an order of magnitude larger:
          </p>
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li>Outperforms <strong>Qwen2.5-VL-72B</strong> by 34% on VSP and 10.67% on BLINK-J</li>
            <li>Surpasses <strong>InternVL3.5-38B</strong> on SAT (52.67% vs. 49.33%)</li>
            <li>Outperforms <strong>GPT-4o</strong> by 24.67% on SAT (52.67% vs. 28.00%)</li>
            <li>Matches <strong>Gemini 2.5 Flash</strong> on general perception in MMVP (80.33%)</li>
          </ul>
          <p style="margin-top: 1rem;">
            Compared across reasoning modes, ThinkMorph's interleaved reasoning consistently outperforms text-only and vision-only approaches by <strong>5.33%</strong>, demonstrating the effectiveness of complementary multimodal reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Interactive Cases -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Interactive Case Gallery</h2>
    <p class="subtitle has-text-centered">
      Click on any case to view detailed reasoning traces
    </p>

    <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Main Reasoning Cases</h3>
    <div class="case-carousel-container" id="main-cases-container">
      <button class="carousel-nav prev" onclick="changeSlide('main-cases', -1)">â€¹</button>
      <div class="case-carousel">
        <div class="case-carousel-track" id="main-cases">
          <!-- Cases will be loaded dynamically -->
        </div>
      </div>
      <button class="carousel-nav next" onclick="changeSlide('main-cases', 1)">â€º</button>
      <div class="carousel-dots" id="main-cases-dots"></div>
      <div class="case-counter" id="main-cases-counter"></div>
    </div>

    <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Unseen Visual Manipulations</h3>
    <div class="case-carousel-container" id="unseen-cases-container">
      <button class="carousel-nav prev" onclick="changeSlide('unseen-cases', -1)">â€¹</button>
      <div class="case-carousel">
        <div class="case-carousel-track" id="unseen-cases">
          <!-- Cases will be loaded dynamically -->
        </div>
      </div>
      <button class="carousel-nav next" onclick="changeSlide('unseen-cases', 1)">â€º</button>
      <div class="carousel-dots" id="unseen-cases-dots"></div>
      <div class="case-counter" id="unseen-cases-counter"></div>
    </div>

    <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Mode Switching Examples</h3>
    <div class="case-carousel-container" id="mode-switch-cases-container">
      <button class="carousel-nav prev" onclick="changeSlide('mode-switch-cases', -1)">â€¹</button>
      <div class="case-carousel">
        <div class="case-carousel-track" id="mode-switch-cases">
          <!-- Cases will be loaded dynamically -->
        </div>
      </div>
      <button class="carousel-nav next" onclick="changeSlide('mode-switch-cases', 1)">â€º</button>
      <div class="carousel-dots" id="mode-switch-cases-dots"></div>
      <div class="case-counter" id="mode-switch-cases-counter"></div>
    </div>
  </div>
</section>

<!-- Modal for case viewing -->
<div class="modal" id="case-modal">
  <div class="modal-background"></div>
  <div class="modal-content modal-content-custom" id="modal-content-container">
    <!-- Content will be dynamically inserted here -->
  </div>
  <button class="modal-close is-large" aria-label="close"></button>
</div>

<!-- Citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{thinkmorph2025,
  title={ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning},
  author={Gu, Jiawei and Hao, Yunzhuo and Wang, Huichen Will and Li, Linjie and Shieh, Michael Qizhe and Choi, Yejin and Krishna, Ranjay and Cheng, Yu},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template adapted from <a href="https://emma-benchmark.github.io/">EMMA</a> and 
        <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

<script>
  // Case gallery data
  const caseData = {
    'main-cases': [
      'case1.pdf', 'case2.pdf', 'case3.pdf', 'case4.pdf', 'case5.pdf'
    ],
    'unseen-cases': [
      'unseen-case-1.pdf', 'unseen-case-2.pdf'
    ],
    'mode-switch-cases': [
      'mode-switch-case-1.pdf', 'mode-switch-case-2.pdf', 'mode-switch-case-3.pdf', 
      'mode-switch-case-4.pdf', 'mode-switch-case-5.pdf'
    ]
  };

  // Track current slide for each carousel
  const carouselState = {
    'main-cases': 0,
    'unseen-cases': 0,
    'mode-switch-cases': 0
  };

  // Initialize carousel
  function initCarousel(carouselId) {
    const container = document.getElementById(carouselId);
    const cases = caseData[carouselId];
    
    // Create case items
    cases.forEach((filename, index) => {
      const caseItem = document.createElement('div');
      caseItem.className = 'case-item';
      
      const img = document.createElement('img');
      img.src = `./static/images/${filename}`;
      img.alt = filename;
      
      caseItem.appendChild(img);
      caseItem.onclick = () => openModal(`./static/images/${filename}`);
      container.appendChild(caseItem);
    });

    // Create dots
    const dotsContainer = document.getElementById(`${carouselId}-dots`);
    cases.forEach((_, index) => {
      const dot = document.createElement('div');
      dot.className = 'carousel-dot' + (index === 0 ? ' active' : '');
      dot.onclick = () => goToSlide(carouselId, index);
      dotsContainer.appendChild(dot);
    });

    // Update counter
    updateCounter(carouselId);
  }

  // Change slide
  function changeSlide(carouselId, direction) {
    const cases = caseData[carouselId];
    const currentIndex = carouselState[carouselId];
    let newIndex = currentIndex + direction;
    
    // Loop around
    if (newIndex < 0) {
      newIndex = cases.length - 1;
    } else if (newIndex >= cases.length) {
      newIndex = 0;
    }
    
    goToSlide(carouselId, newIndex);
  }

  // Go to specific slide
  function goToSlide(carouselId, index) {
    const cases = caseData[carouselId];
    carouselState[carouselId] = index;
    
    // Update track position
    const track = document.getElementById(carouselId);
    track.style.transform = `translateX(-${index * 100}%)`;
    
    // Update dots
    const dots = document.querySelectorAll(`#${carouselId}-dots .carousel-dot`);
    dots.forEach((dot, i) => {
      dot.classList.toggle('active', i === index);
    });

    // Update counter
    updateCounter(carouselId);
  }

  // Update counter
  function updateCounter(carouselId) {
    const cases = caseData[carouselId];
    const currentIndex = carouselState[carouselId];
    const counter = document.getElementById(`${carouselId}-counter`);
    counter.textContent = `${currentIndex + 1} / ${cases.length}`;
  }

  // Initialize all carousels when page loads
  document.addEventListener('DOMContentLoaded', function() {
    initCarousel('main-cases');
    initCarousel('unseen-cases');
    initCarousel('mode-switch-cases');
  });

  // Keyboard navigation
  document.addEventListener('keydown', (e) => {
    if (e.key === 'ArrowLeft' || e.key === 'ArrowRight') {
      const direction = e.key === 'ArrowLeft' ? -1 : 1;
      // You can add logic here to determine which carousel is in view
      // For now, we'll control the first visible carousel
    }
  });

  // Modal functionality
  function openModal(fileSrc) {
    const modal = document.getElementById('case-modal');
    const modalContainer = document.getElementById('modal-content-container');
    
    // Clear previous content
    modalContainer.innerHTML = '';
    
    // Check if file is PDF or image
    if (fileSrc.endsWith('.pdf')) {
      // Display PDF using object tag
      const pdfObject = document.createElement('object');
      pdfObject.data = fileSrc;
      pdfObject.type = 'application/pdf';
      pdfObject.style.width = '100%';
      pdfObject.style.height = '80vh';
      pdfObject.innerHTML = `<p>Your browser doesn't support PDF viewing. <a href="${fileSrc}" target="_blank">Open PDF in new tab</a></p>`;
      modalContainer.appendChild(pdfObject);
    } else {
      // Display image
      const imgElement = document.createElement('img');
      imgElement.src = fileSrc;
      imgElement.alt = 'Case Detail';
      imgElement.style.width = '100%';
      imgElement.style.height = 'auto';
      modalContainer.appendChild(imgElement);
    }
    
    modal.classList.add('is-active');
  }

  function closeModal() {
    const modal = document.getElementById('case-modal');
    modal.classList.remove('is-active');
  }

  // Close modal on background click or close button
  document.querySelector('.modal-background').addEventListener('click', closeModal);
  document.querySelector('.modal-close').addEventListener('click', closeModal);

  // Close modal on Escape key
  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape') {
      closeModal();
    }
  });
</script>

</body>
</html>

