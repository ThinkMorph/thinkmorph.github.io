<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning">
  <meta name="keywords" content="ThinkMorph, Multimodal Reasoning, Chain-of-Thought, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .hero-body {
      padding: 3rem 1.5rem;
    }
    .publication-title {
      margin-bottom: 1rem;
    }
    .case-carousel-container {
      position: relative;
      margin: 2rem auto;
      max-width: 800px;
    }
    .case-carousel {
      overflow: hidden;
      border-radius: 10px;
      box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .case-carousel-track {
      display: flex;
      transition: transform 0.5s ease-in-out;
    }
    .case-item {
      min-width: 100%;
      cursor: pointer;
      position: relative;
      display: flex;
      align-items: center;
      justify-content: center;
      min-height: 400px;
    }
    .case-item img {
      display: block;
      max-width: 100%;
      max-height: 80vh;
      height: auto;
      width: auto;
      border-radius: 10px;
      object-fit: contain;
    }
    .carousel-nav {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      background: rgba(255,255,255,0.9);
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      font-size: 24px;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 2px 8px rgba(0,0,0,0.2);
      z-index: 10;
    }
    .carousel-nav:hover {
      background: white;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .carousel-nav.prev {
      left: -60px;
    }
    .carousel-nav.next {
      right: -60px;
    }
    .carousel-dots {
      display: flex;
      justify-content: center;
      gap: 10px;
      margin-top: 1rem;
    }
    .carousel-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #d1d5db;
      cursor: pointer;
      transition: all 0.3s ease;
    }
    .carousel-dot.active {
      background: #667eea;
      transform: scale(1.2);
    }
    .case-counter {
      text-align: center;
      margin-top: 1rem;
      font-size: 1rem;
      color: #6b7280;
    }
    .modal-content-custom {
      max-width: 90%;
      max-height: 90vh;
      margin: auto;
    }
    .gradient-text {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    /* Navigation styles */
    .navbar-custom {
      background-color: #fff;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      padding: 0.5rem 1rem;
    }
    .navbar-dropdown {
      border: 1px solid #e8e8e8;
      border-radius: 4px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .navbar-item {
      padding: 0.75rem 1rem;
      transition: background-color 0.2s;
    }
    .navbar-item:hover {
      background-color: #f5f5f5;
    }
    .navbar-link {
      font-weight: 500;
      color: #3273dc;
    }
    .navbar-link:hover {
      color: #667eea;
    }
  </style>
</head>
<body>

<!-- Navigation Bar -->
<nav class="navbar navbar-custom" role="navigation" aria-label="main navigation">
  <div class="container is-max-widescreen">
    <div class="navbar-brand">
      <a class="navbar-item" href="index.html">
        <i class="fas fa-home" style="font-size: 1.5rem;"></i>
      </a>
    </div>

    <div class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>

          <div class="navbar-dropdown is-right">
            <a class="navbar-item" href="https://emma-benchmark.github.io/" target="_blank">
              <span class="icon"><i class="fas fa-chart-bar"></i></span>
              <span>EMMA</span>
            </a>
            <a class="navbar-item" href="https://muirbench.github.io/" target="_blank">
              <span class="icon"><i class="fas fa-brain"></i></span>
              <span>MuirBench</span>
            </a>
            <a class="navbar-item" href="https://huggingface.co/datasets/jiaweigu/commonsense-t2i" target="_blank">
              <span class="icon"><i class="fas fa-lightbulb"></i></span>
              <span>Commonsense-T2I</span>
            </a>
            <a class="navbar-item" href="https://visualsketchpad.github.io/" target="_blank">
              <span class="icon"><i class="fas fa-pen"></i></span>
              <span>Visual Sketchpad</span>
            </a>
            <a class="navbar-item" href="https://refocusbench.github.io/" target="_blank">
              <span class="icon"><i class="fas fa-search"></i></span>
              <span>ReFocus</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/thinkmorph_logo_new.png" alt="ThinkMorph Logo" style="max-width: 400px; margin-bottom: 0.1rem;">
          <h1 class="title is-3">Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning</h1>
          <div class="is-size-5 publication-authors" style="margin-top: 2rem;">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7p8yEHAAAAAJ&hl=En">Jiawei Gu</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=0SPl0hcAAAAJ&hl=zh-CN">Yunzhuo Hao</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~wwill/">Huichen Will Wang</a><sup>3,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a><sup>3,*</sup>,</span>
            <span class="author-block">
              <a href="https://michaelshieh.com/">Michael Qizhe Shieh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en">Yejin Choi</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=IcqahyAAAAAJ&hl=en">Ranjay Krishna</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://ych133.github.io/">Yu Cheng</a><sup>5</sup></span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 1rem;">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>Zhejiang University,</span>
            <span class="author-block"><sup>3</sup>University of Washington,</span>
            <span class="author-block"><sup>4</sup>Stanford University,</span>
            <span class="author-block"><sup>5</sup>The Chinese University of Hong Kong</span>
          </div>

          <div class="is-size-6 has-text-centered" style="margin-top: 0.5rem;">
            <p>*Equal contribution</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/ThinkMorph/ThinkMorph" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ThinkMorph" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="font-size:18px">ðŸ¤—</span>
                  <span>Models & Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Main Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/thinkmorph_main.jpg" alt="ThinkMorph Main" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    </div>
  </div>
</section>


<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal reasoning requires <strong>iterative coordination between language and vision</strong>, 
            yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and 
            image thoughts should function as <strong>complementary, rather than isomorphic, modalities</strong> 
            that mutually advance reasoning. 
          </p>
          <div class="hero-body has-text-centered" style="padding: 1.5rem 0;">
            <img src="./static/images/interleaved_design.jpg" alt="Interleaved Design" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>
          <p>
            Guided by this principle, we build <strong>ThinkMorph</strong>, a unified model fine-tuned on 
            <strong>~24K high-quality interleaved reasoning traces</strong> spanning tasks with varying visual 
            engagement. ThinkMorph learns to generate progressive textâ€“image reasoning steps that concretely 
            manipulate visual content while maintaining coherent verbal logic. It delivers <strong class="gradient-text">large gains on vision-centric benchmarks</strong> 
            (averaging <strong>34.7%</strong> over the base model) and generalizes to out-of-domain tasks, 
            matching or surpassing larger and proprietary VLMs. 
            Beyond performance, ThinkMorph exhibits <strong>emergent multimodal intelligence</strong>, including unseen 
            visual manipulation skills, adaptive switching between reasoning modes, and better test-time 
            scaling through diversified multimodal thoughts.
            These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.
          </p>
          <div class="hero-body has-text-centered" style="padding: 1.5rem 0;">
            <img src="./static/images/emrging_prop.jpg" alt="Emergent Properties" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>
          <p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- About ThinkMorph -->
<!-- <section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Introduction</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Multimodal reasoning requires <strong>iterative coordination between language and vision</strong>, 
            yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and 
            image thoughts should function as <strong>complementary, rather than isomorphic, modalities</strong> 
            that mutually advance reasoning. Guided by this principle, we build <strong>ThinkMorph</strong>, a unified model fine-tuned on 
            <strong>~24K high-quality interleaved reasoning traces</strong> spanning tasks with varying visual 
            engagement. 
          </p>
          <div class="hero-body has-text-centered" style="padding: 1.5rem 0;">
            <img src="./static/images/interleaved_design.jpg" alt="Interleaved Design" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>
          <p>
            ThinkMorph learns to generate progressive textâ€“image reasoning steps that concretely 
            manipulate visual content while maintaining coherent verbal logic. It delivers <strong class="gradient-text">large gains on vision-centric benchmarks</strong> 
            (averaging <strong>34.7%</strong> over the base model) and generalizes to out-of-domain tasks, 
            matching or surpassing larger and proprietary VLMs.
          </p>
          <p>
            By fine-tuning with <strong>merely ~24K</strong> samples, ThinkMorph achieves out-of-domain performance that rivals or even surpasses leading large-scale, proprietary VLMs.
          </p>
          <p>
            Intriguingly, ThinkMorph unlocks emergent properties that represent a <em>hallmark of multimodal intelligence</em>: the elicitation of unseen visual manipulation skills, the self-adaptive switching between reasoning modes according to task complexity, and better test-time scaling via diversified thoughts.
          </p>
          <div class="hero-body has-text-centered" style="padding: 1.5rem 0;">
            <img src="./static/images/emrging_prop.jpg" alt="Emergent Properties" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>
          <p>
            These findings suggest promising directions for future work to characterize the emergent capabilities of unified models for multimodal reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Teaser Figure -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/thinkmorph_main_web.png" alt="ThinkMorph Multimodal Reasoning" style="width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    </div>
  </div>
</section> -->

<!-- Method Overview -->
<!-- <section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            ThinkMorph is a unified multimodal model that generates <strong>interleaved textâ€“image reasoning steps</strong> to solve complex multimodal problems. Unlike conventional Chain-of-Thought approaches that only produce textual tokens, ThinkMorph can generate both image tokens and text tokens, resulting in complementary multimodal thoughts that jointly advance reasoning.  We fine-tune ThinkMorph on <strong>~24K high-quality interleaved reasoning traces</strong> spanning four representative tasks with varying levels of visual engagement:
          </p> 
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li><strong>Jigsaw Assembly:</strong> Determining correct arrangement of scrambled image patches</li>
            <li><strong>Spatial Navigation:</strong> Finding safe routes through grid maps with obstacles</li>
            <li><strong>Visual Search:</strong> Locating and identifying target objects in images</li>
            <li><strong>Chart Refocus:</strong> Answering questions about data visualizations</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Visualization -->
<!-- <section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Test-Time Scaling Comparison</h2>
    <div class="hero-body has-text-centered">
      <img src="./static/images/tts_mode_comparison_horizontal_v3.png" alt="Test-Time Scaling" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
      <p class="subtitle has-text-centered" style="margin-top: 1rem;">
        ThinkMorph's interleaved reasoning (green) consistently outperforms text-only (orange) and 
        vision-only (blue) modes across multiple benchmarks during test-time scaling.
      </p>
    </div>
  </div>
</section> -->

<!-- Emergent Properties -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Emergent Properties</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #667eea;">
            <h3 class="title is-4" style="color: #667eea;">Property â‘ : Unseen Visual Manipulations</h3>
            <p>
              ThinkMorph develops accurate and meaningful visual manipulations <strong>unseen in training data</strong> when generalizing to out-of-domain tasks. These include zoom-in operations, inpainting, multi-box generation, motion forecasting, perspective transformation, and region cropping. These emergent behaviors are precise and task-effective, contributing directly to problem solving. For example, when asked "Is the bell pepper red or yellow?", the model automatically generates a zoomed-in view to distinguish subtle color differences, mirroring human visual inspection without explicit prompting.
            </p>
          </div>
          
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #764ba2;">
            <h3 class="title is-4" style="color: #764ba2;">Property â‘¡: Autonomous Mode Switching</h3>
            <p>
              Despite being trained exclusively on interleaved data, ThinkMorph <strong>autonomously switches</strong> to text-only reasoning in 5.3% of inference cases. This switching is task-adaptive, not arbitrary: the model recognizes when fine-grained visual details are essential versus when textual reasoning alone suffices. On questions where the model switches to text-only mode, accuracy improves by 7.29% over interleaved reasoning, demonstrating effective dynamic allocation of reasoning effort based on task demands.
            </p>
          </div>
          
          <div class="box" style="margin-bottom: 2rem; border-left: 4px solid #f093fb;">
            <h3 class="title is-4" style="color: #f093fb;">Property â‘¢: Better Test-Time Scaling via Diversified Thoughts</h3>
            <p>
              Interleaved reasoning enables superior test-time scaling by generating <strong>diversified thoughts</strong> that explore broader multimodal solution spaces. Under Best-of-N sampling, interleaved reasoning consistently outperforms unimodal approaches across all benchmarks, with substantial gains (e.g., +8.0% on BLINK-J). The scaling advantage arises from richer trajectory diversity: while unimodal reasoning chains are confined to single representational spaces, interleaved reasoning explores both modalities simultaneously, greatly improving the likelihood of discovering correct solutions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Results -->
<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <!-- <p>
            ThinkMorph delivers <strong>substantial gains</strong> across diverse vision-centric benchmarks: -->
          <!-- </p>
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li><strong>Average improvement of 34.74%</strong> over the base model Bagel-7B across all benchmarks</li>
            <li><strong>85.84% improvement</strong> on Spatial Navigation (VSP)</li>
            <li><strong>38.75% improvement</strong> on Jigsaw Assembly (VisPuzzle)</li>
            <li><strong>Average gain of 20.74%</strong> over the base model on out-of-domain benchmarks</li>
          </ul>
          <p style="margin-top: 1rem;">
            Despite being fine-tuned on only 24K samples, ThinkMorph achieves performance comparable to, and in several cases exceeding, models an order of magnitude larger:
          </p>
          <ul style="margin-top: 1rem; margin-left: 2rem;">
            <li>Outperforms <strong>Qwen2.5-VL-72B</strong> by 34% on VSP and 10.67% on BLINK-J</li>
            <li>Surpasses <strong>InternVL3.5-38B</strong> on SAT (52.67% vs. 49.33%)</li>
            <li>Outperforms <strong>GPT-4o</strong> by 24.67% on SAT (52.67% vs. 28.00%)</li>
            <li>Matches <strong>Gemini 2.5 Flash</strong> on general perception in MMVP (80.33%)</li>
          </ul> -->
          <p style="margin-top: 2rem;">
            <strong>Reasoning Mode Comparison.</strong>
            ThinkMorph achieves substantial gains on vision-centric tasks, averaging a <strong>34.74% improvement</strong> over its base model, with striking increases of <strong>85.84%</strong> on <em>Spatial Navigation</em> and <strong>38.75%</strong> on <em>Jigsaw Assembly</em>. 
            Compared across reasoning modes, ThinkMorph's interleaved reasoning consistently outperforms text-only and vision-only approaches by <strong>5.33%</strong>.
          </p>
          <div class="hero-body has-text-centered" style="margin-top: 1rem;">
            <img src="./static/images/mode_comparision.png" alt="Mode Comparison" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>

          <p style="margin-top: 2rem;">
            <strong>Comparison with State-of-the-Art VLMs.</strong>
            Compared to its base model, Bagel-7B, ThinkMorph achieves significant improvements across all benchmarks, with an average gain of <strong>20.74%</strong> over nine diverse tasks. Despite being fine-tuned on only <strong>24K samples</strong>, ThinkMorph achieves performance comparable to, and in several cases exceeding, models an order of magnitude larger: it outperforms Qwen2.5-VL-72B by 34% on VSP and 10.67% on BLINK-J, surpasses InternVL3.5-38B on SAT, and matches Gemini 2.5 Flash on general perception in MMVP (80.33%).
          </p>
          <div class="hero-body has-text-centered" style="margin-top: 1.5rem;">
            <img src="./static/images/main_result.png" alt="Main Results" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Additional Figures -->
<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Further Analysis</h2>
    <p class="subtitle has-text-centered">
      Explore more analysis of ThinkMorph's behaviors. For detailed discussions, please refer to our paper.
    </p>

    <div class="case-carousel-container" id="figures-container" style="margin-top: 2rem;">
      <button class="carousel-nav prev" onclick="changeSlide('paper-figures', -1)">â€¹</button>
      <div class="case-carousel">
        <div class="case-carousel-track" id="paper-figures">
          <!-- Figures will be loaded dynamically -->
        </div>
      </div>
      <button class="carousel-nav next" onclick="changeSlide('paper-figures', 1)">â€º</button>
      <div class="carousel-dots" id="paper-figures-dots"></div>
      <div class="case-counter" id="paper-figures-counter"></div>
    </div>
  </div>
</section>

<!-- Interactive Cases -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Interactive Case Gallery</h2>
    <p class="subtitle has-text-centered">
      Click on any case to view detailed reasoning traces
    </p>

    <div class="case-carousel-container" id="all-cases-container" style="margin-top: 2rem;">
      <button class="carousel-nav prev" onclick="changeSlide('all-cases', -1)">â€¹</button>
      <div class="case-carousel">
        <div class="case-carousel-track" id="all-cases">
          <!-- Cases will be loaded dynamically -->
        </div>
      </div>
      <button class="carousel-nav next" onclick="changeSlide('all-cases', 1)">â€º</button>
      <div class="carousel-dots" id="all-cases-dots"></div>
      <div class="case-counter" id="all-cases-counter"></div>
    </div>
  </div>
</section>

<!-- Modal for case viewing -->
<div class="modal" id="case-modal">
  <div class="modal-background"></div>
  <div class="modal-content modal-content-custom" id="modal-content-container">
    <!-- Content will be dynamically inserted here -->
  </div>
  <button class="modal-close is-large" aria-label="close"></button>
</div>

<!-- Citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen">
    <h2 class="title has-text-centered">BibTeX</h2>
    <div class="box" style="position: relative; margin-top: 2.5rem;">

      <!-- Copy Button -->
      <button
        id="copyBibtexBtn"
        class="button is-normal is-info is-rounded"
        aria-label="Copy BibTeX"
        style="position: absolute; top: 0.5rem; right: 0.5rem;"
      >
        <span class="icon"><i class="fas fa-copy"></i></span>
      </button>

      <pre style="overflow-x: auto;"><code id="bibtexCode">@article{thinkmorph2025,
  title={ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning},
  author={Gu, Jiawei and Hao, Yunzhuo and Wang, Huichen Will and Li, Linjie and Shieh, Michael Qizhe and Choi, Yejin and Krishna, Ranjay and Cheng, Yu},
  journal={arXiv preprint},
  year={2025}
}</code></pre>

    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

<script>
  // Case gallery data
  const caseData = {
    'paper-figures': [
      // Main figures from paper (excluding Figure 1 & 2)
      'boundary_case_01.png',
      'more_unseen_01.png',
      'case-mode-switch_01.png',
      'tts_mode_comparison_horizontal_v3.png',
      'tts_percentage_improvement_v2_01.png',
      'mode_distribution_01.png',
    ],
    'all-cases': [
      // Individual case examples
      'case1_01.png',
      'case2_01.png',
      'case3_01.png',
      'case4_01.png',
      'case5_01.png',
      'mode-switch-case-1_01.png',
      'mode-switch-case-2_01.png',
      'mode-switch-case-3_01.png',
      'mode-switch-case-4_01.png',
      'mode-switch-case-5_01.png',
      'unseen-case-2_01.png',
      'unseen-case-9-motion_01.png',
      'unseen-case-11-image_01.png',
      'unseen-case-15-Eliminate_01.png',
      'unseen-case-16-zoom_01.png',
      'unseen-case-19-inpainting_01.png'
    ]
  };

  // Track current slide for each carousel
  const carouselState = {
    'paper-figures': 0,
    'all-cases': 0
  };

  // Initialize carousel
  function initCarousel(carouselId) {
    const container = document.getElementById(carouselId);
    const cases = caseData[carouselId];
    
    // Create case items
    cases.forEach((filename, index) => {
      const caseItem = document.createElement('div');
      caseItem.className = 'case-item';
      
      const img = document.createElement('img');
      img.src = `./static/images/${filename}`;
      img.alt = filename;
      
      caseItem.appendChild(img);
      caseItem.onclick = () => openModal(`./static/images/${filename}`);
      container.appendChild(caseItem);
    });

    // Create dots
    const dotsContainer = document.getElementById(`${carouselId}-dots`);
    cases.forEach((_, index) => {
      const dot = document.createElement('div');
      dot.className = 'carousel-dot' + (index === 0 ? ' active' : '');
      dot.onclick = () => goToSlide(carouselId, index);
      dotsContainer.appendChild(dot);
    });

    // Update counter
    updateCounter(carouselId);
  }

  // Change slide
  function changeSlide(carouselId, direction) {
    const cases = caseData[carouselId];
    const currentIndex = carouselState[carouselId];
    let newIndex = currentIndex + direction;
    
    // Loop around
    if (newIndex < 0) {
      newIndex = cases.length - 1;
    } else if (newIndex >= cases.length) {
      newIndex = 0;
    }
    
    goToSlide(carouselId, newIndex);
  }

  // Go to specific slide
  function goToSlide(carouselId, index) {
    const cases = caseData[carouselId];
    carouselState[carouselId] = index;
    
    // Update track position
    const track = document.getElementById(carouselId);
    track.style.transform = `translateX(-${index * 100}%)`;
    
    // Update dots
    const dots = document.querySelectorAll(`#${carouselId}-dots .carousel-dot`);
    dots.forEach((dot, i) => {
      dot.classList.toggle('active', i === index);
    });

    // Update counter
    updateCounter(carouselId);
  }

  // Update counter
  function updateCounter(carouselId) {
    const cases = caseData[carouselId];
    const currentIndex = carouselState[carouselId];
    const counter = document.getElementById(`${carouselId}-counter`);
    counter.textContent = `${currentIndex + 1} / ${cases.length}`;
  }

  // Initialize all carousels when page loads
  document.addEventListener('DOMContentLoaded', function() {
    initCarousel('paper-figures');
    initCarousel('all-cases');
  });

  // Keyboard navigation
  document.addEventListener('keydown', (e) => {
    if (e.key === 'ArrowLeft' || e.key === 'ArrowRight') {
      const direction = e.key === 'ArrowLeft' ? -1 : 1;
      // You can add logic here to determine which carousel is in view
      // For now, we'll control the first visible carousel
    }
  });

  // Modal functionality
  function openModal(fileSrc) {
    const modal = document.getElementById('case-modal');
    const modalContainer = document.getElementById('modal-content-container');
    
    // Clear previous content
    modalContainer.innerHTML = '';
    
    // Check if file is PDF or image
    if (fileSrc.endsWith('.pdf')) {
      // Display PDF using object tag
      const pdfObject = document.createElement('object');
      pdfObject.data = fileSrc;
      pdfObject.type = 'application/pdf';
      pdfObject.style.width = '100%';
      pdfObject.style.height = '80vh';
      pdfObject.innerHTML = `<p>Your browser doesn't support PDF viewing. <a href="${fileSrc}" target="_blank">Open PDF in new tab</a></p>`;
      modalContainer.appendChild(pdfObject);
    } else {
      // Display image
      const imgElement = document.createElement('img');
      imgElement.src = fileSrc;
      imgElement.alt = 'Case Detail';
      imgElement.style.width = '100%';
      imgElement.style.height = 'auto';
      modalContainer.appendChild(imgElement);
    }
    
    modal.classList.add('is-active');
  }

  function closeModal() {
    const modal = document.getElementById('case-modal');
    modal.classList.remove('is-active');
  }

  // Close modal on background click or close button
  document.querySelector('.modal-background').addEventListener('click', closeModal);
  document.querySelector('.modal-close').addEventListener('click', closeModal);

  // Close modal on Escape key
  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape') {
      closeModal();
    }
  });

  // Copy BibTeX functionality
  document.getElementById('copyBibtexBtn').addEventListener('click', async () => {
    const code = document.getElementById('bibtexCode').innerText.trim();
    try {
      await navigator.clipboard.writeText(code);
      // Show success message
      const btn = document.getElementById('copyBibtexBtn');
      const originalHTML = btn.innerHTML;
      btn.innerHTML = '<span class="icon"><i class="fas fa-check"></i></span>';
      btn.classList.remove('is-info');
      btn.classList.add('is-success');
      
      setTimeout(() => {
        btn.innerHTML = originalHTML;
        btn.classList.remove('is-success');
        btn.classList.add('is-info');
      }, 2000);
    } catch (err) {
      console.error('Copy failed', err);
      alert('Copy failed, please copy manually.');
    }
  });
</script>

</body>
</html>

